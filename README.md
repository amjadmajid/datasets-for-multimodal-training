# Datasets and Simulators for Training Robotic Multimodal Foundational Models


Below is a curated selection of the most-used & recent **robot-centric data sets that power generative-AI research in robotics**.  The table lists each data set’s year, scope and scale, and the licence that governs redistribution or commercial use.  This should let you pick a corpus whose terms match your project (e.g., Apache 2.0 or CC-BY for commercial reuse vs. “research-only” agreements).

## Embodied & manipulation-focused datasets

| Dataset                                                                                                                                 | Year | Task(s)                                                             | Scale                                                                                                                 | Licence                                                                                                                                      |
| --------------------------------------------------------------------------------------------------------------------------------------- | ---- | ------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| <a href="https://huggingface.co/datasets/agibot-world/AgiBotWorld-Beta" target="_blank" rel="noopener noreferrer">AgiBot World Beta</a> | 2025 | Multi-robot, multi-domain manipulation                              | \~1.0 M trajectories; 100 robots; 100+ replicated real-world scenarios                                                | CC-BY-NC-SA 4.0                                                                                                                              |
| <a href="https://tuwien-asl.github.io/REASSEMBLE_page/" target="_blank" rel="noopener noreferrer">REASSEMBLE</a>                        | 2025 | Contact-rich assembly & disassembly (pick/insert/remove/place)      | 4 551 demonstrations (4 035 successful) with event cameras, force-torque, mics & multi-view RGB                       | CC-BY 4.0                                                                                                                                    |
| <a href="https://github.com/SiamiLab/RoboMNIST" target="_blank" rel="noopener noreferrer">RoboMNIST</a>                                 | 2025 | Multi-robot activity recognition via WiFi, video & audio            | Synchronized data from two Franka arms, three cameras, three WiFi sniffers & three microphones                        | CC-BY 4.0                                                                                                                                    |
| <a href="https://arxiv.org/abs/2505.21495" target="_blank" rel="noopener noreferrer">CLAMP</a>                                          | 2025 | In-the-wild haptic perception (low-cost reacher-grabber)            | 12.3 M datapoints across 5 357 household objects (seven sensing modalities)                                           | CC-BY 4.0                                                                                                                                    |
| <a href="https://huggingface.co/blog/lerobot-datasets" target="_blank" rel="noopener noreferrer">LeRobot Community</a>                  | 2025 | Community-contributed multimodal robot data (arms, mobile, driving) | 487 datasets · ≈ 10 M frames                                                                                          | Varied (mostly Apache 2.0)                                                                                                                   |
| <a href="https://ivl.cs.brown.edu/research/gigahands.html" target="_blank" rel="noopener noreferrer">GigaHands</a>                      | 2025 | Bimanual hand activities (motion synthesis)                         | 34 h from 56 subjects & 417 objects; 14 k clips (183 M frames) · 84 k text annotations                                | Not yet specified                                                                                                                            |
| <a href="https://github.com/M2RL/m2rl-dataset" target="_blank" rel="noopener noreferrer">M2RL</a>                                       | 2024 | Multi-interface robot learning from human demos                     | \~360 trajectories (\~1 M image frames) over 8 tasks; RGB-D (3 views), operator ego view & gaze, robot proprioception | Not specified                                                                                                                                |
| <a href="https://functional-manipulation-benchmark.github.io/" target="_blank" rel="noopener noreferrer">FMB</a>                        | 2024 | Functional, long-horizon manipulation                               | 22 550 expert trajectories                                                                                            | <a href="https://functional-manipulation-benchmark.github.io/" target="_blank" rel="noopener noreferrer">CC-BY 4.0</a>                       |
| <a href="https://mobile-aloha.github.io/" target="_blank" rel="noopener noreferrer">Mobile ALOHA</a>                                    | 2024 | Bimanual mobile manipulation                                        | ≈ 50 demos / task (5+ tasks)                                                                                          | <a href="https://github.com/MarkFzp/mobile-aloha/blob/main/LICENSE" target="_blank" rel="noopener noreferrer">MIT</a>                        |
| <a href="https://droid-dataset.github.io/" target="_blank" rel="noopener noreferrer">DROID</a>                                          | 2024 | In-the-wild manipulation                                            | 76 k trajectories · 564 scenes · 86 tasks                                                                             | <a href="https://github.com/droid-dataset/droid_policy_learning/blob/master/LICENSE" target="_blank" rel="noopener noreferrer">MIT</a>       |
| <a href="https://robotics-transformer-x.github.io/" target="_blank" rel="noopener noreferrer">RT-X / Open X-Embodiment</a>              | 2023 | Multi-embodiment, multi-task manipulation                           | ≈ 1 M+ trajectories · 160 k tasks · 22 robot types                                                                    | <a href="https://github.com/google-deepmind/open_x_embodiment/blob/main/LICENSE" target="_blank" rel="noopener noreferrer">Apache 2.0</a>    |
| <a href="https://robopen.github.io/roboset/" target="_blank" rel="noopener noreferrer">RoboSet</a>                                      | 2023 | Household kitchen manipulation                                      | 30 050 trajectories · 38 tasks                                                                                        | <a href="https://github.com/robopen/roboagent/blob/main/LICENSE" target="_blank" rel="noopener noreferrer">MIT</a>                           |
| <a href="https://github.com/google-research/language-table" target="_blank" rel="noopener noreferrer">Language Table</a>                | 2023 | Table-top rearrangement via language                                | ≈ 600 k labelled trajectories                                                                                         | <a href="https://github.com/google-research/language-table/blob/main/LICENSE" target="_blank" rel="noopener noreferrer">Apache 2.0</a>       |
| <a href="https://vimalabs.github.io/" target="_blank" rel="noopener noreferrer">VIMA</a>                                                | 2023 | Simulated multimodal manipulation                                   | ≈ 650 k trajectories                                                                                                  | <a href="https://huggingface.co/datasets/VIMA/VIMA-Data" target="_blank" rel="noopener noreferrer">CC-BY 4.0</a>                             |
| <a href="https://rail-berkeley.github.io/bridgedata/" target="_blank" rel="noopener noreferrer">BridgeData V2</a>                       | 2023 | Tele-operated & scripted manipulation                               | 60 096 trajectories (50 365 tele-op + 9 731 scripted) across 24 envs & 13 skills                                      | CC-BY 4.0                                                                                                                                    |
| <a href="https://robotics-transformer1.github.io/" target="_blank" rel="noopener noreferrer">RT-1</a>                                   | 2022 | Multi-task real-world manipulation                                  | ≈ 130 k episodes · 700 tasks                                                                                          | <a href="https://github.com/google-research/robotics_transformer/blob/main/LICENSE" target="_blank" rel="noopener noreferrer">Apache 2.0</a> |
| <a href="https://sites.google.com/view/bc-z/home" target="_blank" rel="noopener noreferrer">BC-Z</a>                                    | 2021 | Diverse manipulation                                                | ≈ 39 k trajectories · 100+ tasks                                                                                      | <a href="https://www.kaggle.com/datasets/google/bc-z-robot" target="_blank" rel="noopener noreferrer">CC-BY 4.0</a>                          |
| <a href="https://roboturk.stanford.edu/" target="_blank" rel="noopener noreferrer">RoboTurk</a>                                         | 2019 | Crowd-sourced manipulation demos                                    | 2 144 demonstrations · 3 tasks                                                                                        | <a href="https://github.com/RoboTurk-Platform/roboturk_real_dataset/blob/master/LICENSE" target="_blank" rel="noopener noreferrer">MIT</a>   |


## Embodied & manipulation-focused datasets

| Dataset                                                                                                                                 | Year | Task(s)                                                             | Scale                                                                                                                 | Licence                                                                                                                                      |
| --------------------------------------------------------------------------------------------------------------------------------------- | ---- | ------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| <a href="https://huggingface.co/datasets/agibot-world/AgiBotWorld-Beta" target="_blank" >AgiBot World Beta</a> | 2025 | Multi-robot, multi-domain manipulation                              | \~1.0 M trajectories; 100 robots; 100+ replicated real-world scenarios                                                | CC-BY-NC-SA 4.0                                                                                                                              |
| <a href="https://tuwien-asl.github.io/REASSEMBLE_page/" target="_blank" >REASSEMBLE</a>                        | 2025 | Contact-rich assembly & disassembly (pick/insert/remove/place)      | 4 551 demonstrations (4 035 successful) with event cameras, force-torque, mics & multi-view RGB                       | CC-BY 4.0                                                                                                                                    |
| <a href="https://github.com/SiamiLab/RoboMNIST" target="_blank" >RoboMNIST</a>                                 | 2025 | Multi-robot activity recognition via WiFi, video & audio            | Synchronized data from two Franka arms, three cameras, three WiFi sniffers & three microphones                        | CC-BY 4.0                                                                                                                                    |
| <a href="https://arxiv.org/abs/2505.21495" target="_blank" >CLAMP</a>                                          | 2025 | In-the-wild haptic perception (low-cost reacher-grabber)            | 12.3 M datapoints across 5 357 household objects (seven sensing modalities)                                           | CC-BY 4.0                                                                                                                                    |
| <a href="https://huggingface.co/blog/lerobot-datasets" target="_blank" >LeRobot Community</a>                  | 2025 | Community-contributed multimodal robot data (arms, mobile, driving) | 487 datasets · ≈ 10 M frames                                                                                          | Varied (mostly Apache 2.0)                                                                                                                   |
| <a href="https://ivl.cs.brown.edu/research/gigahands.html" target="_blank" >GigaHands</a>                      | 2025 | Bimanual hand activities (motion synthesis)                         | 34 h from 56 subjects & 417 objects; 14 k clips (183 M frames) · 84 k text annotations                                | Not yet specified                                                                                                                            |
| <a href="https://github.com/M2RL/m2rl-dataset" target="_blank" >M2RL</a>                                       | 2024 | Multi-interface robot learning from human demos                     | \~360 trajectories (\~1 M image frames) over 8 tasks; RGB-D (3 views), operator ego view & gaze, robot proprioception | Not specified                                                                                                                                |
| <a href="https://functional-manipulation-benchmark.github.io/" target="_blank" >FMB</a>                        | 2024 | Functional, long-horizon manipulation                               | 22 550 expert trajectories                                                                                            | <a href="https://functional-manipulation-benchmark.github.io/" target="_blank" >CC-BY 4.0</a>                       |
| <a href="https://mobile-aloha.github.io/" target="_blank" >Mobile ALOHA</a>                                    | 2024 | Bimanual mobile manipulation                                        | ≈ 50 demos / task (5+ tasks)                                                                                          | <a href="https://github.com/MarkFzp/mobile-aloha/blob/main/LICENSE" target="_blank" >MIT</a>                        |
| <a href="https://droid-dataset.github.io/" target="_blank" >DROID</a>                                          | 2024 | In-the-wild manipulation                                            | 76 k trajectories · 564 scenes · 86 tasks                                                                             | <a href="https://github.com/droid-dataset/droid_policy_learning/blob/master/LICENSE" target="_blank" >MIT</a>       |
| <a href="https://robotics-transformer-x.github.io/" target="_blank" >RT-X / Open X-Embodiment</a>              | 2023 | Multi-embodiment, multi-task manipulation                           | ≈ 1 M+ trajectories · 160 k tasks · 22 robot types                                                                    | <a href="https://github.com/google-deepmind/open_x_embodiment/blob/main/LICENSE" target="_blank" >Apache 2.0</a>    |
| <a href="https://robopen.github.io/roboset/" target="_blank" >RoboSet</a>                                      | 2023 | Household kitchen manipulation                                      | 30 050 trajectories · 38 tasks                                                                                        | <a href="https://github.com/robopen/roboagent/blob/main/LICENSE" target="_blank" >MIT</a>                           |
| <a href="https://github.com/google-research/language-table" target="_blank" >Language Table</a>                | 2023 | Table-top rearrangement via language                                | ≈ 600 k labelled trajectories                                                                                         | <a href="https://github.com/google-research/language-table/blob/main/LICENSE" target="_blank" >Apache 2.0</a>       |
| <a href="https://vimalabs.github.io/" target="_blank" >VIMA</a>                                                | 2023 | Simulated multimodal manipulation                                   | ≈ 650 k trajectories                                                                                                  | <a href="https://huggingface.co/datasets/VIMA/VIMA-Data" target="_blank" >CC-BY 4.0</a>                             |
| <a href="https://rail-berkeley.github.io/bridgedata/" target="_blank" >BridgeData V2</a>                       | 2023 | Tele-operated & scripted manipulation                               | 60 096 trajectories (50 365 tele-op + 9 731 scripted) across 24 envs & 13 skills                                      | CC-BY 4.0                                                                                                                                    |
| <a href="https://robotics-transformer1.github.io/" target="_blank" >RT-1</a>                                   | 2022 | Multi-task real-world manipulation                                  | ≈ 130 k episodes · 700 tasks                                                                                          | <a href="https://github.com/google-research/robotics_transformer/blob/main/LICENSE" target="_blank" >Apache 2.0</a> |
| <a href="https://sites.google.com/view/bc-z/home" target="_blank" >BC-Z</a>                                    | 2021 | Diverse manipulation                                                | ≈ 39 k trajectories · 100+ tasks                                                                                      | <a href="https://www.kaggle.com/datasets/google/bc-z-robot" target="_blank" >CC-BY 4.0</a>                          |
| <a href="https://roboturk.stanford.edu/" target="_blank" >RoboTurk</a>                                         | 2019 | Crowd-sourced manipulation demos                                    | 2 144 demonstrations · 3 tasks                                                                                        | <a href="https://github.com/RoboTurk-Platform/roboturk_real_dataset/blob/master/LICENSE" target="_blank" >MIT</a>   |



## Navigation & embodied-vision datasets

| Dataset                                                                                                                        | Year    | Domain                                        | Scale / Sensors                                                                                           | Licence                                                                                                                                        |
| ------------------------------------------------------------------------------------------------------------------------------ | ------- | --------------------------------------------- | --------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| <a href="https://github.com/suhaisheng/RoboSense" target="_blank">RoboSense</a>                      | 2025    | Egocentric near-field perception & navigation | 133 k synchronized frames; 1.4 M 3-D boxes; 216 k trajectories                                            | <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" >CC-BY-NC-SA 4.0</a>                     |
| <a href="https://tartanair.org/tartanground/" target="_blank" >TartanGround</a>                       | 2025    | Ground-robot perception & navigation          | 910 trajectories (\~1.5 M samples); 6 RGB stereo pairs (360°), depth, semantics, LiDAR, IMU, optical-flow | <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" >CC-BY 4.0</a>                                 |
| <a href="https://sites.google.com/view/egowalk" target="_blank" >EgoWalk</a>                          | 2025    | Human navigation (indoor/outdoor)             | 50 h egocentric nav data; stereo cameras, odometry; NL goal annotations & traversability masks            | <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" >CC-BY 4.0</a>                                 |
| <a href="https://arxiv.org/abs/2505.07446" target="_blank" >TPT-Bench</a>                             | 2025    | Target-person tracking                        | 48 sequences (\~3.35 h); odometry, 3-D LiDAR, IMU, panoramic & RGB-D                                      | <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" >CC-BY-SA 4.0</a>                           |
| <a href="https://pengyu-team.github.io/S3E/" target="_blank" >S3E</a>                                 | 2024    | Collaborative multi-robot SLAM                | 13 outdoor & 5 indoor sequences; 360° LiDAR, stereo RGB, IMU, UWB                                         | <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" >CC-BY 4.0</a>                                 |
| <a href="https://cs.gmu.edu/~xiao/Research/GND/" target="_blank" >Global Navigation Dataset (GND)</a> | 2024    | Campus-scale navigation                       | > 11 h across \~2.7 km²; 3-D LiDAR, RGB, 360° camera, IMU, GPS; traversability maps                       | <a href="https://creativecommons.org/publicdomain/zero/1.0/" target="_blank" >CC0 1.0</a>                             |
| <a href="https://nvidia-ai-iot.github.io/remembr/" target="_blank" >NaVQA</a>                         | 2025    | Navigation video question answering           | 210 long-horizon navigation videos with spatial/temporal/descriptive Q\&A                                 | Not specified                                                                                                                                  |
| <a href="https://rail-berkeley.github.io/bridgedata/" target="_blank" >BridgeData V2</a>              | 2023    | Manipulation (tele-op nav & pick-and-place)   | 60 096 trajectories; 3 camera views/demo; natural-language labels                                         | <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" >CC-BY 4.0</a>                                 |
| <a href="https://aihabitat.org/datasets/hm3d/" target="_blank" >HM3D</a>                              | 2021    | 3-D indoor navigation                         | 1 000 scanned buildings                                                                                   | <a href="https://github.com/facebookresearch/habitat-matterport3d-dataset/blob/main/LICENSE" target="_blank" >MIT</a> |
| <a href="https://gibsonenv.stanford.edu/" target="_blank" >Gibson</a>                                 | 2018    | Real-scan navigation                          | 572 scenes                                                                                                | <a href="https://github.com/StanfordVL/GibsonEnv/blob/master/LICENSE" target="_blank" >MIT</a>                        |
| <a href="https://bringmeaspoon.org/" target="_blank" >Room-to-Room (R2R)</a>                          | 2018    | Vision-language navigation                    | 22 k human instructions                                                                                   | <a href="https://bringmeaspoon.org/" target="_blank" >Custom research-only</a>                                        |
| <a href="https://ai2thor.allenai.org/" target="_blank" >AI2-THOR / RoboTHOR</a>                       | 2017-24 | Interactive navigation & manipulation         | 200+ synthetic rooms; RoboTHOR adds 89 apartments                                                         | <a href="https://github.com/allenai/ai2thor/blob/main/LICENSE" target="_blank" >Apache 2.0</a>                        |


---

## Simulators for embodied multimodal models

| Name | Task | Scenes | Sensors | Platform | Year | Licence |
| --- | --- | --- | --- | --- | --- | --- |
| <a href="https://arxiv.org/abs/2405.14045" target="_blank" >SDF-Sim</a> | Action, Navigation | – | RGB | – | 2024 | TBD — no public repo yet |
| <a href="https://arxiv.org/abs/2403.08629" target="_blank" >TRUMANS</a> | Action, Navigation | 100 indoor scenes | VICON · RGB-D · IMU | A800 GPU | 2024 | <a href="https://github.com/cornellsml/truman/blob/master/LICENSE" target="_blank" >MIT</a> |
| <a href="https://github.com/KovenYu/WonderWorld" target="_blank" >WonderWorld</a> | Action, Navigation | – | – | A6000 GPU · AR/VR | 2024 | TBD — licence not yet posted |
| <a href="https://craigleili.github.io/projects/genzi/" target="_blank" >GenZI</a> | Action, Navigation | – | – | A100 GPU | 2024 | TBD — code forthcoming |
| <a href="http://svl.stanford.edu/igibson/" target="_blank" >iGibson 2.0</a> | Action, Navigation | 15 scenes (108 rooms) | RGB · Depth · LiDAR | GTX 1080 Ti · VR HMDs · Fetch | 2021 | <a href="https://github.com/StanfordVL/iGibson/blob/master/LICENSE" target="_blank" >MIT</a> |
| <a href="https://github.com/facebookresearch/habitat-sim" target="_blank" >Habitat-Sim</a> | Action, Navigation | – | RGB · Depth · GPS · Compass · Contact | Titan Xp · VR | 2019 | <a href="https://github.com/facebookresearch/habitat-sim/blob/main/LICENSE" target="_blank" >MIT</a> |
| <a href="https://github.com/Genesis-Embodied-AI/Genesis" target="_blank" >Genesis</a> | All | All | – | – | 2024 | <a href="https://github.com/Genesis-Embodied-AI/Genesis/blob/main/LICENSE" target="_blank" >Apache 2.0</a> |
| <a href="https://github.com/peteanderson80/Matterport3DSimulator" target="_blank" >Matterport3D Sim</a> | Navigation | 90 buildings | RGB-D · Panoramic | Titan Xp | 2017 | <a href="https://github.com/peteanderson80/Matterport3DSimulator/blob/master/LICENSE" target="_blank" >MIT</a> |
| <a href="https://soundspaces.org/" target="_blank" >SoundSpaces</a> | Navigation | 103 scenes · 102 sounds | RGB-D · Mic | – | 2020 | <a href="https://github.com/facebookresearch/soundspaces-challenge/blob/main/LICENSE" target="_blank" >MIT</a> |
| <a href="https://github.com/facebookresearch/sound-spaces" target="_blank" >SoundSpaces v2</a> | Navigation | – | RGB-D · Mic | – | 2022 | MIT (same repo) |
| <a href="https://ai2thor.allenai.org/" target="_blank" >AI2-THOR</a> | Action, Navigation, Manipulation | 120 synthetic rooms | RGB · Depth · Semantic | Unity-based | 2017 | <a href="https://github.com/allenai/ai2thor/blob/main/LICENSE" target="_blank" >Apache 2.0</a> |
| <a href="https://github.com/ARISE-Initiative/robosuite" target="_blank" >RoboSuite</a> | Robotic manipulation | 25 MuJoCo tasks | RGB · Depth | GPU (MuJoCo) | 2020 | <a href="https://github.com/ARISE-Initiative/robosuite/blob/master/LICENSE" target="_blank" >Apache 2.0</a> |
| <a href="https://gazebosim.org/libs/sim/" target="_blank" >Gazebo Sim (gz-sim)</a> | Manipulation, Navigation, Multi-robot | User-defined | Any Gazebo sensor | CPU/GPU | 2023 | <a href="https://github.com/gazebosim/gz-sim/blob/gz-sim9/LICENSE" target="_blank" >Apache 2.0</a> |
| <a href="https://pybullet.org/" target="_blank" >PyBullet</a> | Physics, Manipulation | User-defined | OpenGL · Force/Torque | CPU/GPU | 2017 | <a href="https://github.com/bulletphysics/bullet3/blob/master/LICENSE.txt" target="_blank" >zlib/libpng</a> |
| <a href="https://carla.org/" target="_blank" >CARLA</a> | Autonomous driving | 20 + urban maps | RGB · LiDAR · RADAR | GPU (UE 4) | 2017 | <a href="https://github.com/carla-simulator/carla/blob/master/LICENSE" target="_blank" >MIT</a> |
| <a href="https://github.com/threedworld-mit/tdw" target="_blank" >ThreeDWorld (TDW)</a> | Multi-modal physics, Embodied AI | 1000s objects / layouts | RGB-D · Audio · Force | GPU (Unity) | 2021 | <a href="https://github.com/threedworld-mit/tdw/blob/master/LICENSE.txt" target="_blank" >BSD-2-Clause</a> |
| <a href="https://developer.nvidia.com/isaac/sim" target="_blank" >Isaac Sim</a> | Action, Navigation, Manipulation, Synthetic-data | Pre-built + custom USD worlds | RGB · Depth (LiDAR/Radar) · IMU · Contact | RTX-class GPU · Omniverse | 2021 | <a href="https://docs.omniverse.nvidia.com/isaacsim/latest/common/NVIDIA_Omniverse_License_Agreement.html" target="_blank" >NVIDIA Omniverse EULA</a> |



### Reading the licences

* **Apache 2.0 / MIT** – permissive; redistribution (even commercial) is fine with attribution.
* **CC-BY 4.0** – also permissive but requires explicit attribution for any reuse, including derivatives.
* **Custom / research-only** – typically bars commercial use and redistribution without permission; check each EULA carefully.
